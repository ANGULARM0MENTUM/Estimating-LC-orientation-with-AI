# director_field_estimator.py
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms
from PIL import Image
import numpy as np
import pyvista as pv
from tqdm import tqdm

import matplotlib.pyplot as plt

def visualize_prediction(image_tensor, pred_tensor, target=None, idx=0):
    """
    Old-style visualization, fixed for sharp 40×40 display
    """
    # Input image
    img = image_tensor.cpu().permute(1, 2, 0).numpy()
    img = img * 0.5 + 0.5   # denormalize
    img = np.clip(img, 0, 1)
    
    # Predicted director → HSV (angle = hue)
    pred = pred_tensor.cpu().permute(1, 2, 0).numpy()  # [H, W, 2]
    angle = np.arctan2(pred[:,:,1], pred[:,:,0]) % np.pi
    hue = angle / np.pi
    sat = np.ones_like(hue)
    val = np.ones_like(hue)
    hsv = np.stack([hue, sat, val], axis=-1)
    
    # Use hsv_to_rgb to avoid alpha channel completely
    from matplotlib.colors import hsv_to_rgb
    pred_rgb = hsv_to_rgb(hsv)  # shape (H, W, 3) — no alpha
    
    # Plot
    num_plots = 2 if target is None else 3
    fig, axes = plt.subplots(1, num_plots, figsize=(4 * num_plots, 4))  # smaller for 40×40
    
    axes[0].imshow(img, interpolation='none')  # no blur
    axes[0].set_title(f"Input patch {idx}")
    axes[0].axis('off')
    
    axes[1].imshow(pred_rgb, interpolation='none')
    axes[1].set_title("Predicted director field")
    axes[1].axis('off')
    
    if target is not None:
        tgt = target.cpu().permute(1, 2, 0).numpy()
        t_angle = np.arctan2(tgt[:,:,1], tgt[:,:,0]) % np.pi
        t_hue = t_angle / np.pi
        t_hsv = np.stack([t_hue, np.ones_like(t_hue), np.ones_like(t_hue)], -1)
        tgt_rgb = hsv_to_rgb(t_hsv)
        axes[2].imshow(tgt_rgb, interpolation='none')
        axes[2].set_title("Ground truth")
        axes[2].axis('off')
    
    plt.tight_layout()
    plt.show()

# ─── Device ────────────────────────────────────────────────────────────────
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# ─── 1. Dataset ─────────────────────────────────────────────────────────────
class DirectorDataset(Dataset):
    def __init__(self, root_dir, patch_size=40, transform=None):
        self.root_dir = root_dir
        self.patch_size = patch_size
        self.transform = transform
        
        # Find all .bmp files, sort numerically
        bmp_files = [f for f in os.listdir(root_dir) if f.lower().endswith('.bmp')]
        bmp_files.sort(key=lambda x: int(os.path.splitext(x)[0]))
        self.image_files = bmp_files
        
        print(f"Found {len(self.image_files)} full 160×160 BMP images")
        print("Will generate multiple 40×40 patches per image")

    def __len__(self):
        # For simplicity: 1 patch per image (top-left)
        # If you want 16 patches per image → return len(self.image_files) * 16
        return len(self.image_files)   # ← change to *16 later if you want

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.root_dir, img_name)
        
        # Load full 160×160 image
        image_full = Image.open(img_path).convert('RGB')
        
        # Crop top-left 40×40 patch (you can randomize or loop later)
        patch_size = self.patch_size
        left, upper = 0, 0  # ← top-left corner
        # For center patch: left = (160 - patch_size) // 2, upper = same
        
        image_crop = image_full.crop((left, upper, left + patch_size, upper + patch_size))
        
        if self.transform:
            image = self.transform(image_crop)
        else:
            image = transforms.ToTensor()(image_crop)
        
        if image.shape != torch.Size([3, 40, 40]):
            raise ValueError(f"Image patch shape wrong: {image.shape}")
        
        # Load full director field
        base = os.path.splitext(img_name)[0]
        vti_path = os.path.join(self.root_dir, f"{base}.vti")
        
        if not os.path.exists(vti_path):
            raise FileNotFoundError(f"Missing .vti: {vti_path}")
        
        mesh = pv.read(vti_path)
        array_name = 'n'
        
        if array_name not in mesh.point_data:
            raise KeyError(f"Array '{array_name}' not found. Available: {list(mesh.point_data.keys())}")
        
        data = mesh.point_data[array_name]
        dims = mesh.dimensions
        
        if len(dims) == 3 and dims[2] == 1:
            data = data.reshape(dims[1], dims[0], -1)
        else:
            mid_z = dims[2] // 2
            data = data.reshape(dims[2], dims[1], dims[0], -1)[mid_z]
        
        if data.shape[-1] == 3:
            target_full = torch.from_numpy(data[..., :2].astype(np.float32))
            target_full = target_full.permute(2, 0, 1)  # [2, H, W]
        elif data.shape[-1] == 2:
            target_full = torch.from_numpy(data.astype(np.float32)).permute(2, 0, 1)
        else:
            raise ValueError(f"Unexpected data shape: {data.shape}")
        
        # Crop the same 40×40 region from target
        target = target_full[:, upper:upper+patch_size, left:left+patch_size]
        
        # Normalize
        norm = torch.norm(target, p=2, dim=0, keepdim=True) + 1e-8
        target = target / norm
        
        if target.shape != torch.Size([2, 40, 40]):
            raise ValueError(f"Target patch shape wrong: {target.shape}")
        
        return image, target


# ─── 2. Transforms ──────────────────────────────────────────────────────────
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)
])


# ─── 3. Model ───────────────────────────────────────────────────────────────
class MiniUNetDirector(nn.Module):
    def __init__(self, in_channels=3, out_channels=2):
        super().__init__()
        
        def conv_block(in_c, out_c):
            return nn.Sequential(
                nn.Conv2d(in_c, out_c, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_c, out_c, 3, padding=1),
                nn.ReLU(inplace=True)
            )
        
        # Encoder
        self.enc1 = conv_block(in_channels, 64)
        self.enc2 = conv_block(64, 128)
        self.enc3 = conv_block(128, 256)
        
        # Bottleneck
        self.bottleneck = conv_block(256, 512)
        
        # Decoder - concatenated channels
        self.dec3 = conv_block(512 + 256, 256)   # 768 → 256
        self.dec2 = conv_block(256 + 128, 128)   # 384 → 128
        self.dec1 = conv_block(128 + 64, 64)     # 192 → 64
        
        self.final = nn.Conv2d(64, out_channels, 1)

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(F.max_pool2d(e1, 2))
        e3 = self.enc3(F.max_pool2d(e2, 2))
        
        b = self.bottleneck(F.max_pool2d(e3, 2))
        
        # Decoder
        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)
        d3 = torch.cat([d3, e3], dim=1)
        d3 = self.dec3(d3)
        
        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)
        
        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1)
        
        out = self.final(d1)
        out = F.normalize(out, p=2, dim=1)
        return out


# ─── 4. Loss ────────────────────────────────────────────────────────────────
def director_loss(pred, target):
    cos_sim = torch.sum(pred * target, dim=1).clamp(-1.0 + 1e-6, 1.0 - 1e-6)
    
    # Softer angular loss (linear at first)
    angular_loss = torch.mean(1 - cos_sim)  # classic
    # Optional: quadratic only on large errors
    # angular_loss = torch.mean(torch.relu(1 - cos_sim - 0.5) ** 2) + torch.mean(1 - cos_sim) * 0.5
    
    mse = F.mse_loss(pred, target)
    
    return 1.0 * angular_loss + 0.2 * mse


# ─── 5. Main ────────────────────────────────────────────────────────────────
def main():
    folder = "C:/Users/user/Downloads/FYP/Director_simulations/Test"   # ← CHANGE THIS
    dataset = DirectorDataset(root_dir=folder, transform=transform)
    
    print("Total samples in dataset:", len(dataset))
    print("First 5 filenames:", dataset.image_files[:5] if hasattr(dataset, 'image_files') else "No image_files list")
    print("Last 5 filenames:", dataset.image_files[-5:] if hasattr(dataset, 'image_files') else "No image_files list")

    # Force load a few samples and check uniqueness
    seen_inputs = set()
    for i in range(min(10, len(dataset))):
        img, tgt = dataset[i]
        img_hash = hash(tuple(img.flatten().tolist()))  # rough hash to detect duplicates
        seen_inputs.add(img_hash)
        print(f"Sample {i}: image shape {img.shape}, hash {img_hash}")

    print(f"Number of unique inputs in first 10 samples: {len(seen_inputs)}")
    
    if len(dataset) == 0:
        raise ValueError("No data loaded")
    
    train_size = int(0.85 * len(dataset))
    val_size = len(dataset) - train_size
    train_ds, val_ds = random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,
                              num_workers=0, pin_memory=True)
    val_loader = DataLoader(val_ds, batch_size=16, shuffle=False,
                            num_workers=0, pin_memory=True)
    
    model = MiniUNetDirector().to(device)
    optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
    
    NUM_EPOCHS = 50
    
    for epoch in range(NUM_EPOCHS):
        model.train()
        train_loss = 0.0
        
        for images, targets in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
            images = images.to(device)
            targets = targets.to(device)
        
            optimizer.zero_grad()
            preds = model(images)
            # Encourage direction variation across the patch
            var_loss = -torch.mean(torch.var(preds.view(preds.size(0), 2, -1), dim=2)) * 0.05
            loss = director_loss(preds, targets) + var_loss
            preds = F.normalize(preds, p=2, dim=1)  # safe
        
            loss = director_loss(preds, targets)
            if torch.isnan(loss):
                print("NaN loss detected! Skipping backward.")
                continue
        
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        
            train_loss += loss.item()
    
        train_loss /= len(train_loader)
    
        # Validation
        model.eval()
        val_loss = 0.0
        total_mae = 0.0
        num_batches = 0

        with torch.no_grad():
            for images, targets in val_loader:
                images = images.to(device)
                targets = targets.to(device)
                preds = model(images)
                loss = director_loss(preds, targets)
                val_loss += loss.item()
                # MAE calculation
                cos_sim = torch.sum(preds * targets, dim=1).clamp(-1.0 + 1e-6, 1.0 - 1e-6)
                angle_rad = torch.acos(cos_sim)
                mae_deg = torch.mean(angle_rad * 180 / np.pi).item()
                total_mae += mae_deg
                num_batches += 1
        val_loss /= len(val_loader)
        scheduler.step(val_loss)
        avg_mae = total_mae / num_batches if num_batches > 0 else 0.0
        print(f"Epoch [{epoch+1}/{NUM_EPOCHS}] | "
          f"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}" 
          f" MAE: {avg_mae:.2f}°")
        
        
        # if epoch % 2 == 0 or epoch == NUM_EPOCHS - 1:  # every 5 epochs or last one
        #    model.eval()
        #    with torch.no_grad():
        #        # Take one batch from validation
        #        images, targets = next(iter(val_loader))
        #        images = images.to(device)
        #        preds = model(images)
        #        # Visualize first sample in the batch
        #        visualize_prediction(
        #            images[0].cpu(),          # first image
        #            preds[0].cpu(),           # first prediction
        #            targets[0].cpu(),         # ground truth (if available)
        #            idx=epoch                 # show epoch number
        #        )

    torch.save(model.state_dict(), "director_model.pth")
    print("Done.")
    
    print("Training finished. Visualizing some predictions...")
    print("Predicted norm min/max:", torch.norm(preds, dim=1).min().item(), torch.norm(preds, dim=1).max().item())
    print("Predicted cos_sim with a constant vector (e.g. [1,0]):",
          torch.mean(torch.sum(preds * torch.tensor([1.0, 0.0]).view(1,2,1,1).to(device), dim=1)).item())
    
    model.eval()
    with torch.no_grad():
        vis_loader = DataLoader(val_ds, batch_size=1, shuffle=True)  # fresh shuffled loader
        for i in range(5):
            images, targets = next(iter(vis_loader))
            images = images.to(device)
            preds = model(images)
            preds = F.normalize(preds, p=2, dim=1)
        
            visualize_prediction(
                images[0].cpu(),
                preds[0].cpu(),
                targets[0].cpu(),
                idx=f"Validation sample {i}"
            )
    print("Predicted norm min/max:", torch.norm(preds, dim=1).min().item(), torch.norm(preds, dim=1).max().item())
    print("Predicted cos_sim with a constant vector (e.g. [1,0]):",
          torch.mean(torch.sum(preds * torch.tensor([1.0, 0.0]).view(1,2,1,1).to(device), dim=1)).item())

if __name__ == '__main__':
    main()
    
